---
title: "LinearModelGenerator"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LinearModelGenerator}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# WELCOME
First of all, you can using this download command to download this packages and vignettes,
```{r,eval=FALSE}
devtools::install_github('ZihaoHanGitHub/Biostat625_Generate_Linear_Model',build_vignettes =T)
```
Secondly, you can using library to call out this package.
```{r setup}
library(LinearModelGenerator)
library(rbenchmark)
```
In this tutorial, it is divided into four parts: 

- **_Data set Setup:_**: For using the data set in this package;
- **_Generate Linear Model:_**: For using the `generate_linear_model()` function to fit the model with your argument and data set;
- **_Get Prediction:_**: For using the `get_pred()` function to calculate the response variable value for the new dataset, especially for an unknown response variable;
- **_Compare the packages with original R function:_**: In this section, we use the functions inside the packages to compare with the R function, assessing them for correctness and efficiency.

# Dataset Setup
In this packages, it contain a dataset about depression, with four variables, Depression, Age, Sex, and Fatalism, it is common to use for a linear regression model calculating, thus, feel good to practice this packages via this dataset.
```{r}
data = LinearModelGenerator::data
head(data)
```
We could see, this data frame contain 612 rows and 4 columns, with four vairables called Depression, Age, Sex and Fatalism, in this tutorial, this dataset is applied for all the example of function using.

# Generate Linear Model

This function is designed by vector operation to speed up the calculation, and aim to solve the multiple linear regression model, hypothesis testing, and several information related to the MLR.

## How to use this function?

After we set up the data, we can using the generate linear model with a string (the formula of the model you want to fit), and the dataset, here is the example,
```{r}
# suppose you want to fit the linear model, 
#explore the relationship between Depression with Age, Sex, and Fatalism.
model = generate_linear_model(Depression ~ Age + Sex + Fatalism, data)
```
Thus, you would fit a multiple linear model by Least Squared Estimate, and all the information would save in the variable $model$, in the section of output, it would be introduce how to use this $model$.

## What is this function output?

This function would return a list with five things, a mathematics express of this model, a coefficient table, a R squared table, a F-test table, a 95% confident interval of this table, and also a Sum of Sq table.

### The Mathematics Formula & Coefficient Table

First of all, let's have a overview of this $model$.
```{r}
model
```
Then let's see how to call out each attribute.
```{r}
#Call out the mathematics formula
#it would return a formula type data
math_express = model$Mathematics_formula
math_express
```
Then, let's check the coefficient table
```{r}
# the coefficient table is a dataframe class
# contain these five columns name,
# Estimate: the value of parameter
# Std. Error: the standard error of parameter
# t-value: the t-test value of this parameter
# p-value: the p_value of the t test corresponding.
coefficient_table = model$coefficients_table
coefficient_table
```
And it is easily to call each colunms of coefficient table,
```{r}
# Example:
# using the estimate value
# for other attributes of coefficient table would be similar to call out.
Estimate = coefficient_table[,"Estimate"]
Estimate
```

### SS table, $R^2$table,& F-test table

This part is introduce how to call the SS table, $R^2$ table and F-test table of the model, first, it is going to introduce the Sum Sq table, For Sum Sq table, you can applied this command
```{r}
# call out SS_table
SS_table = model$SS_table
SS_table
```
We could see it is a dataframe containing the SSE,dfE,MSE, and other two rows for Regression term and Y term. Next, it introduce how to call the R squared table, which is a dataframe contains the $R^2$ and $adj R^2$, for the $R^2$ table, you can run this code.
```{r}
R_table = model$Rsquared_table
R_table
```
In the end of this section, it introduce how to call the F-test table, which is a dataframe contains the F-statistics, Degree of Freedom1, Degree of Freedom2, and also the p-value.
```{r}
F_table = model$F_table
F_table
```

### Confidence Interval

This section is going to introduce about the 95%CI of each parameter, you can call out this data frame by the code following:
```{r}
# CI is the data frame type
CI = model$confident_interval
CI
```
You could see the upper bound and lower bound as teh columns name, and intercept and your dependent variables are the row names, therefore, as the dataframe type, you can save the upper bound or the confidence interval for the specific variable by following code:
```{r}
#Call the upper bound of 95% confidence interval
CI[,"confident_interval_upper"]
#Call the 95% confidence interval for intercept
CI["(Intercept)",]
```

## get Prediction

This function is designed for predicting the value of new dataset by the model you already fitted by the generate_linear_model, for example, for the code above, we already explore the relationship between Depression with Age, Sex, and Fatalism, if there is three participants with their Age, Sex, and Fatalism score, how can ewe predict the depression to each of them? Let's generate the new dataset first,

```{r}
# generate three participants information, make it a dataframe, called new_X
new_X <- data.frame(Age = c(15, 29, 34), Sex = c(1, 0, 1), Fatalism = c(2, 0, 5))
```
Then, it would helpful to use the get_pred function to predict the Depression score for each of these three participants.
```{r}
# Input the model (return by generate_linear_model()) and new_X to the get_pred function
get_pred(model,new_X)
```
Hence, you can get the predict value of these three participants!

# Compare with Original R function

Proof this package valid is necessary to compare with the original R function, each parts of function output would be compare to the original function.
First of all the test, let's fit a linear model by orginal R function.
```{r}
# Fit the model by lm(), with same argument and dataset
model_lm = lm(Depression ~ Age + Sex + Fatalism, data=data)
summary_lm = summary(model_lm)
summary_lm
```

## Test the coefficient table

```{r}
# We already see that the significant level of each parameter are same
# let's check other estimate, Std. Error, t value, and p-value
# call out the coefficient table of packages, and convert to the packages formatt
coefficient_table = model$coefficients_table
coefficients = coefficient_table[,c("Estimate","Std. Error", "t value", "Pr(>|t|)")]
coefficients = as.matrix(coefficients)
# call out the coefficient table of R function
coefficient_table_lm = summary_lm$coefficients
```
Let's check whether they are same:
```{r}
library(testthat)
expect_equal(coefficients,coefficient_table_lm)
```

## Test the R squared table

For coefficient table and significant level check, it pass all of the value check, let's go ahead to check the $R^2$ table.
```{r}
# generate the R square and adj R square of package
R_squared = R_table$R_squared
adjR_squared = R_table$Adjusted_R_squared
# generate the R square and adj R square of R function
R_squared_lm = summary_lm$r.squared
adj_r_squared_lm <- summary_lm$adj.r.squared
```
Now, check whether they are same,
```{r}
expect_equal(R_squared,R_squared_lm)
expect_equal(adjR_squared,adj_r_squared_lm)
```

## Test the F-test

From the R squared test part, it already pass the checking, it continued with F-table check
```{r}
# lets convert the F-table to the R original function format
F_table_sub = F_table[,c("value","numdf","dendf")]
F_table_sub = as.matrix(F_table_sub)
p_value = F_table[,"p_value"]
# generate the F-table by the R original function
F_table_lm = summary(model_lm)$fstatistic
f_test_result = anova(model_lm)
p_value_lm = pf(F_table_lm[1], F_table_lm[2], F_table_lm[3], lower.tail = FALSE)
#Let's check whether they are same
expect_equal(t(F_table_sub), as.matrix(F_table_lm))
expect_equal(p_value,as.numeric(p_value_lm))
```

## test the Sum Sq table

The F-table pass the test, it continues with the sum of squared table test
```{r}
# Let's calculate the Sum Sq table of R original Function
# First calculate the Residuals
residuals_lm <- residuals(model_lm)
# Calculate the SSE
SSE_lm = sum(residuals_lm^2)
# Calculate the TRUE value of SSY
mean_y = mean(data$Depression)
SSY_lm = sum((data$Depression - mean_y)^2)
# Calculate the TRUE value of SSR, because SSR = SSY - SSE
SSR_lm = SSY_lm - SSE_lm
# put SSR, SSY, SSE together to test
SS_lm = c(SSR_lm,SSE_lm,SSY_lm)
SS = SS_table$SS
expect_equal(SS,SS_lm)
```
The SSE, SSR, SSY pass the test, let's calcualte the degree of freedom for each of these three
```{r}
# Lets calculate by the dataset
n = nrow(data)
p = nrow(coefficient_table_lm)
# calculate the dfE, dfR, dfY
dfE_lm = n - p
dfR_lm = p - 1
dfY_lm = n - 1
df_lm = c(dfR_lm,dfE_lm,dfY_lm)
# call out the value of this packages generate
df = SS_table$df
expect_equal(df,df_lm)
```
Since the $MSE =\frac{SSE}{dfE}$,$MSR =\frac{SSR}{dfR}$, and $MSY =\frac{SSY}{dfY}$, all of the SSE,SSY,SSR and the degree of freedom of each of them pass the value test, the MSE, MSY, MSR would be same.

## Test the Confidence Interval

Since the SS table pass the check, there are only one part in generate linear model functino left for testing, let's continue to test the confidence interval (on .95 confidence).
```{r}
# call the lower bound and upper bound
lower_bond = CI[, 1]
upper_bond = CI[, 2]
# convert to the same format
lower_bond = as.matrix(lower_bond)
upper_bond = as.matrix(upper_bond)
rownames(lower_bond) = c("(Intercept)","Age","Sex","Fatalism")
rownames(upper_bond) = c("(Intercept)","Age","Sex","Fatalism")
# generate the 95CI of original R function
conf_interval_true <- confint(model_lm)
lower_bond_lm = conf_interval_true[, 1]
upper_bond_lm = conf_interval_true[, 2]
# check whether of them are equal
expect_equal(lower_bond, as.matrix(lower_bond_lm))
expect_equal(upper_bond, as.matrix(upper_bond_lm))
```

## Test on get_pred 

The test of generate_linear_model is already passed, it is neccessary to do the same test on the get_pred function, first of all, generate the new dataset for testing.
```{r}
# generate the new data frame containing the Age, Sex, and Fatalism
new_X1 <- data.frame(Age = c(15, 29, 34), Sex = c(1, 0, 1), Fatalism = c(2, 0, 5))
new_X2 <- data.frame(Age = c(11, 23, 39), Sex = c(0, 0, 1), Fatalism = c(3, 9, 10))
new_X3 <- data.frame(Age = c(84, 24, 74), Sex = c(1, 1, 1), Fatalism = c(13, 6, 12))
# Predict by get_pred and Original R function
predict1 = get_pred(model,new_X1)
predict_lm1 = predict(model_lm, newdata = new_X1)
predict2 = get_pred(model,new_X2)
predict_lm2 = predict(model_lm, newdata = new_X2)
predict3 = get_pred(model,new_X3)
predict_lm3 = predict(model_lm, newdata = new_X3)
```
The value is already calculated, let's check whether they are same!
```{r}
# check whether, they are same on value
expect_equal(predict1, as.matrix(predict_lm1),ignore_attr = TRUE)
expect_equal(predict2, as.matrix(predict_lm2),ignore_attr = TRUE)
expect_equal(predict3, as.matrix(predict_lm3),ignore_attr = TRUE)
```

## Compare the efficient with Original R function

It would be better using the rbenchmark to compare the speed of function by same dataset and argument.
```{r}
# Checking the generate_linear_model() vs. lm()
# repeating 500 times
benchmark(LinearModelGenerator={
           model_packages = generate_linear_model(Depression~Age+Sex+Fatalism,data)
         }, 
         OriginalRCode = {
           model_lm =lm(Depression~Age+Sex+Fatalism, data = data)
         }, 
          replications = 500, columns = c("test", "replications", "elapsed", "relative", "user.self", "sys.self"))
```
Let's continue check the speed of get_pred() function compare to the predict(),
```{r}
# Checking the get_pred() vs. prediction()
# repeating 500 times
subdata = data[,c("Age","Sex","Fatalism")]
benchmark(Get_Predict={
           Y = get_pred(model,subdata)
         }, 
         OriginalRCode = {
           Y_lm =predict(model_lm, newdata = subdata)
         }, 
          replications = 500, columns = c("test", "replications", "elapsed", "relative", "user.self", "sys.self"))
```
Overall, the speed of get_pred() function is faster than original R code, but the generate_linear_model() is slower than the R code, maybe it is because I integrated many outputs as my final return in the function. During the execution of this function, it processes a much larger amount of output compared to the original R code.
